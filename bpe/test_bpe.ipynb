{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e4967227-6851-457d-8488-d6543578a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始文档库\n",
    "vocab={'h o l y </w>': 5, 'h o l i e r </w>': 2, 'n e w s t </w>': 6, 'w i d e s t </w>':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21498ab3-8feb-4500-bae2-7651e63f16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有的单词拆分成单个字符，并且在最后添加停止符\u0001</w>，同时标记词频\n",
    "import collections\n",
    "def get_tokens(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "#tokens = get_tokens(vocab)\n",
    "#print('Tokens: {}'.format(tokens))\n",
    "#print('Number of tokens: {}'.format(len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8001ea68-d6e2-4f17-94fd-3c01ea1a35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第3.1步：统计词典中连续字节对的出现频率，代码实现和执行一次的结果如下所示，\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    print(\"vocab:\",vocab)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        #print(\"symbols:\",symbols)\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "efbaaa0d-7870-4e03-8c18-09349ecc88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs = get_stats(vocab)\n",
    "# print('pairs: {}'.format(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fcf5e120-d574-49a5-b1ff-70a94c12845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到最高频率的连续字节对\n",
    "# best = max(pairs, key=pairs.get)\n",
    "# print('Best pair: {} count:{}'.format(best,pairs[best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f5c5d240-167f-4f87-8444-a6a1bf247d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 第3.3步：合成新的subword，代码实现和合成后的结果如下所示，通过正则匹配，将vocab中指定连续字节对进行合并。\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "28b6d7bf-f5a1-4323-bbf4-47e7b5c09599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = merge_vocab(best, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "022eacfc-6109-4171-899b-01ff141d4bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h o l y </w>': 5, 'h o l i e r </w>': 2, 'n e w s t </w>': 6, 'w i d e s t </w>': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f397a1b7-cec3-4008-bb6c-917aae0634f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "vocab: {'h o l y </w>': 5, 'h o l i e r </w>': 2, 'n e w s t </w>': 6, 'w i d e s t </w>': 3}\n",
      "Best pair: ('s', 't') count:9\n",
      "new vocab: {'h o l y </w>': 5, 'h o l i e r </w>': 2, 'n e w st </w>': 6, 'w i d e st </w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'h': 7, 'o': 7, 'l': 7, 'y': 5, '</w>': 16, 'i': 5, 'e': 11, 'r': 2, 'n': 6, 'w': 9, 'st': 9, 'd': 3})\n",
      "Number of tokens: 12 \n",
      "\n",
      "\n",
      "iter: 1\n",
      "vocab: {'h o l y </w>': 5, 'h o l i e r </w>': 2, 'n e w st </w>': 6, 'w i d e st </w>': 3}\n",
      "Best pair: ('st', '</w>') count:9\n",
      "new vocab: {'h o l y </w>': 5, 'h o l i e r </w>': 2, 'n e w st</w>': 6, 'w i d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'h': 7, 'o': 7, 'l': 7, 'y': 5, '</w>': 7, 'i': 5, 'e': 11, 'r': 2, 'n': 6, 'w': 9, 'st</w>': 9, 'd': 3})\n",
      "Number of tokens: 12 \n",
      "\n",
      "\n",
      "iter: 2\n",
      "vocab: {'h o l y </w>': 5, 'h o l i e r </w>': 2, 'n e w st</w>': 6, 'w i d e st</w>': 3}\n",
      "Best pair: ('h', 'o') count:7\n",
      "new vocab: {'ho l y </w>': 5, 'ho l i e r </w>': 2, 'n e w st</w>': 6, 'w i d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'ho': 7, 'l': 7, 'y': 5, '</w>': 7, 'i': 5, 'e': 11, 'r': 2, 'n': 6, 'w': 9, 'st</w>': 9, 'd': 3})\n",
      "Number of tokens: 11 \n",
      "\n",
      "\n",
      "iter: 3\n",
      "vocab: {'ho l y </w>': 5, 'ho l i e r </w>': 2, 'n e w st</w>': 6, 'w i d e st</w>': 3}\n",
      "Best pair: ('ho', 'l') count:7\n",
      "new vocab: {'hol y </w>': 5, 'hol i e r </w>': 2, 'n e w st</w>': 6, 'w i d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'hol': 7, 'y': 5, '</w>': 7, 'i': 5, 'e': 11, 'r': 2, 'n': 6, 'w': 9, 'st</w>': 9, 'd': 3})\n",
      "Number of tokens: 10 \n",
      "\n",
      "\n",
      "iter: 4\n",
      "vocab: {'hol y </w>': 5, 'hol i e r </w>': 2, 'n e w st</w>': 6, 'w i d e st</w>': 3}\n",
      "Best pair: ('n', 'e') count:6\n",
      "new vocab: {'hol y </w>': 5, 'hol i e r </w>': 2, 'ne w st</w>': 6, 'w i d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'hol': 7, 'y': 5, '</w>': 7, 'i': 5, 'e': 5, 'r': 2, 'ne': 6, 'w': 9, 'st</w>': 9, 'd': 3})\n",
      "Number of tokens: 10 \n",
      "\n",
      "\n",
      "iter: 5\n",
      "vocab: {'hol y </w>': 5, 'hol i e r </w>': 2, 'ne w st</w>': 6, 'w i d e st</w>': 3}\n",
      "Best pair: ('ne', 'w') count:6\n",
      "new vocab: {'hol y </w>': 5, 'hol i e r </w>': 2, 'new st</w>': 6, 'w i d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'hol': 7, 'y': 5, '</w>': 7, 'i': 5, 'e': 5, 'r': 2, 'new': 6, 'st</w>': 9, 'w': 3, 'd': 3})\n",
      "Number of tokens: 10 \n",
      "\n",
      "\n",
      "iter: 6\n",
      "vocab: {'hol y </w>': 5, 'hol i e r </w>': 2, 'new st</w>': 6, 'w i d e st</w>': 3}\n",
      "Best pair: ('new', 'st</w>') count:6\n",
      "new vocab: {'hol y </w>': 5, 'hol i e r </w>': 2, 'newst</w>': 6, 'w i d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'hol': 7, 'y': 5, '</w>': 7, 'i': 5, 'e': 5, 'r': 2, 'newst</w>': 6, 'w': 3, 'd': 3, 'st</w>': 3})\n",
      "Number of tokens: 10 \n",
      "\n",
      "\n",
      "iter: 7\n",
      "vocab: {'hol y </w>': 5, 'hol i e r </w>': 2, 'newst</w>': 6, 'w i d e st</w>': 3}\n",
      "Best pair: ('hol', 'y') count:5\n",
      "new vocab: {'holy </w>': 5, 'hol i e r </w>': 2, 'newst</w>': 6, 'w i d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'holy': 5, '</w>': 7, 'hol': 2, 'i': 5, 'e': 5, 'r': 2, 'newst</w>': 6, 'w': 3, 'd': 3, 'st</w>': 3})\n",
      "Number of tokens: 10 \n",
      "\n",
      "\n",
      "iter: 8\n",
      "vocab: {'holy </w>': 5, 'hol i e r </w>': 2, 'newst</w>': 6, 'w i d e st</w>': 3}\n",
      "Best pair: ('holy', '</w>') count:5\n",
      "new vocab: {'holy</w>': 5, 'hol i e r </w>': 2, 'newst</w>': 6, 'w i d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'holy</w>': 5, 'hol': 2, 'i': 5, 'e': 5, 'r': 2, '</w>': 2, 'newst</w>': 6, 'w': 3, 'd': 3, 'st</w>': 3})\n",
      "Number of tokens: 10 \n",
      "\n",
      "\n",
      "iter: 9\n",
      "vocab: {'holy</w>': 5, 'hol i e r </w>': 2, 'newst</w>': 6, 'w i d e st</w>': 3}\n",
      "Best pair: ('w', 'i') count:3\n",
      "new vocab: {'holy</w>': 5, 'hol i e r </w>': 2, 'newst</w>': 6, 'wi d e st</w>': 3}\n",
      "Tokens: defaultdict(<class 'int'>, {'holy</w>': 5, 'hol': 2, 'i': 2, 'e': 5, 'r': 2, '</w>': 2, 'newst</w>': 6, 'wi': 3, 'd': 3, 'st</w>': 3})\n",
      "Number of tokens: 10 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 综合以上\n",
    "for iter in range(10):\n",
    "    print(\"iter:\", iter)\n",
    "    # 统计词典中连续字节对的出现频率\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    print('Best pair: {} count:{}'.format(best,pairs[best]))\n",
    "    # 成新的subword，代码实现和合成后的结果如下所示，通过正则匹配，将vocab中指定连续字节对进行合并\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('new vocab:',vocab)\n",
    "    # # 将所有的单词拆分成单个字符，同时标记词频(注意：这里每次计算token其实比较低效)\n",
    "    tokens = get_tokens(vocab)\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5f7a9374-2148-4a38-868b-fa1e50f9e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码的原理同样比较拗口，这里直接介绍代码实现形式，之后会系统性总结原理。假如现在有一个词典如下，输入的待编码单词是'moutain</w>'，\n",
    "vocab = {'n': 4150, 's</w>': 4698, 'in': 3363, 'ta': 1009, 'ou': 3936, 'm': 7476}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "606ba9f1-c67e-4ef9-86b3-89c349b6bac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_tokens: ['s</w>', 'ou', 'in', 'ta', 'm', 'n']\n"
     ]
    }
   ],
   "source": [
    "# 第1步：先按照字符数量对词典中的token长度进行倒排。\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "\n",
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "    sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]),\n",
    "                                 reverse=True)\n",
    "    return [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "vocab = {'n': 4150, 's</w>': 4698, 'in': 3363, 'ta': 1009, 'ou': 3936, 'm': 7476}\n",
    "sorted_tokens = get_tokens_from_vocab(vocab)\n",
    "print(\"sorted_tokens:\",sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3a7fb6a0-31cc-4784-b5b0-788b756f77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第2步：从左到右拆分待编码单词，使得拆分后的子串总数尽量少，且子串都在词典中出现过。\n",
    "详细流程为：待编码单词从左到右迭代，在词典中依次找到目标token，满足条件为尽量长且能作为待编码单词的子串。如果在词典中找不到作为待编码单词子串的token，就用<unk>候补。这样找到的子串组合即为待编码单词的编码结果。\n",
    "如果采用暴力遍历的方式，算法时间复杂度非常高，BPE的做法是采用中序遍历解决问题，具体实现方式如下所示。\n",
    "\"\"\"\n",
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "    print(\"sorted_tokens:\",sorted_tokens)\n",
    "    string_tokens = []\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        print(\"token_reg:\", token_reg, \" matched_positions:\", matched_positions)\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        print(\"substring_end_positions:\", substring_end_positions) # 其实list的长度=1\n",
    "        # 整体逻辑为中序遍历\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            ## 先遍历左子树\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i + 1:],\n",
    "                                           unknown_token=unknown_token)\n",
    "\n",
    "            ## 后遍历根节点\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "\n",
    "        ## 最后遍历右子树\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i + 1:],\n",
    "                                       unknown_token=unknown_token)\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1fc86ffd-2eac-4aaf-b96d-cb3e4dc0500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_tokens: ['s</w>', 'ou', 'in', 'ta', 'm', 'n']\n",
      "token_reg: s</w>  matched_positions: [(10, 15)]\n",
      "substring_end_positions: [10]\n",
      "sorted_tokens: ['ou', 'in', 'ta', 'm', 'n']\n",
      "token_reg: ou  matched_positions: [(3, 5)]\n",
      "substring_end_positions: [3]\n",
      "sorted_tokens: ['in', 'ta', 'm', 'n']\n",
      "token_reg: in  matched_positions: [(0, 2)]\n",
      "substring_end_positions: [0]\n",
      "sorted_tokens: ['ta', 'm', 'n']\n",
      "token_reg: ta  matched_positions: []\n",
      "token_reg: m  matched_positions: [(0, 1)]\n",
      "substring_end_positions: [0]\n",
      "sorted_tokens: ['in', 'ta', 'm', 'n']\n",
      "token_reg: in  matched_positions: [(3, 5)]\n",
      "substring_end_positions: [3]\n",
      "sorted_tokens: ['ta', 'm', 'n']\n",
      "token_reg: ta  matched_positions: [(1, 3)]\n",
      "substring_end_positions: [1]\n",
      "sorted_tokens: ['m', 'n']\n",
      "token_reg: m  matched_positions: []\n",
      "token_reg: n  matched_positions: [(0, 1)]\n",
      "substring_end_positions: [0]\n"
     ]
    }
   ],
   "source": [
    "# 这里总结编码的整体流程：\n",
    "#对词典中token按照字符长度进行倒排\n",
    "#从左到右拆分待编码单词，使得拆分后的子串总数尽量少，且子串都在词典中出现过。拆分后的子串组合即为待编码单词的编码结果。\n",
    "\n",
    "word_given = 'inmountains</w>'\n",
    "encodes = tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2232d3d8-76e0-452a-baf4-20e61a13865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encodes: ['in', 'm', 'ou', 'n', 'ta', 'in', 's</w>']\n"
     ]
    }
   ],
   "source": [
    "print(\"encodes:\",encodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6edc20-aa7b-4682-8e19-027e8aa9b32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3b012-3195-43b1-89a8-61bd7ca2cc40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
